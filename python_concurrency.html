<html>
  <head>
    <title>Python Concurrency</title>
  </head>
  <body>
    <style>
      .content {
        max-width: 700px;
        margin: auto;
      }
    </style>
    <div class="content">
      <h1>
        Concurrency in Python
      </h1>
      <p>By Edward D'Souza (May 9th, 2020)</p>
      <hr />
      <h2>Introduction</h2>
      <p>
        I've always been afraid of concurrency in Python. Threading isn't
        supposed to be a good idea because of the GIL. Multi-processing sounds
        complicated. In this article I look at what happpens if you try
        concurrency wiith a simple, computationally intensive task. I found that
        parallelizing tasks with multiple processes is both effective and
        surpisingly simple in Python.
      </p>
      <h2>Example Problem</h2>
      <p>
        We are going to explore concurrency in Python using this example
        problem:
      </p>
      <p>
        Given a set of 2D points ({(x0, y0), (x1, y1), etc.}), find the closest
        pair.
      </p>
      <p>
        This problem has a O(n*log(n)) solution, but to keep things simple,
        we're going to use the naive O(n^2) solution.
      </p>
      <h2>Basic algorithm</h2>
      <p>
        Before getting into concurrency, let's start with the basic solution to
        this problem.
      </p>
      <p>
        To make things more flexible later, we're going to structure it as a
        function that takes in two sets of points ("from" and "to") and finds
        the closest pair of points between the two sets. To find the closest
        pair within an entire set of points, we can pass in the same set of
        points to both arguments.
      </p>
      <figure>
        <figcaption>main.py</figcaption>
        <pre>
          <code>
from algorithm import closest_pair

points = [(0,1), (9, 10), (2, 3)]
print(closest_pair(points, points))  # --> ((0, 1), (2, 3))
          </code>
        </pre>
      </figure>
      <figure>
        <figcaption>algorithm.py</figcaption>
        <pre>
          <code>
import itertools
import math
from random import randrange, seed; seed(0)


def closest_pair(from_set, to_set):
    closest_pair = None
    closest_distance = math.inf

    for a in from_set:
        for b in to_set:
            if a != b:
                ab_distance = distance(a, b)
                if ab_distance < closest_distance:
                    closest_pair = (a, b)
                    closest_distance = ab_distance

    return closest_pair


def distance(a, b):  # https://docs.python.org/3.8/library/math.html#math.dist
    return math.sqrt(sum((ad - bd) ** 2 for ad, bd in zip(a, b)))


NUM_POINTS = 3500

def generate_points(num_points=NUM_POINTS):
    return [(randrange(0, 100), randrange(0, 100)) for _ in range(num_points)]
          </code>
        </pre>
      </figure>
      <h2>Single-threaded performance</h2>
      <p>
        As a baseline, let's look at the how long it takes the algorithm to
        complete using a single thread. Note that I'm using NUM_POINTS = 3500
        for all the examples to make the problem sizable enough to measure on my
        machine.
      </p>
      <figure>
        <figcaption>single_threaded.py</figcaption>
        <pre>
          <code>
from algorithm import closest_pair, generate_points

points = generate_points()
closest = closest_pair(points, points)
print(closest)
          </code>
        </pre>
      </figure>
      <figure>
        <figcaption>single_threaded.py results</figcaption>
        <pre>
          <code>
$ time python single_thread1.py
((49, 97), (49, 96))

real    0m18.904s
user    0m18.665s
sys     0m0.056s
          </code>
        </pre>
      </figure>
      <h2>Multi-threaded performance</h2>
      <p>
        In a CPU bound task, there shouldn't be any benefit to using multiple
        threads, because of the
        <a
          href="https://docs.python.org/3/glossary.html#term-global-interpreter-lock"
          >GIL</a
        >. But we're going to do it anyway just to see what happens.
      </p>
      <figure>
        <figcaption>multi_threaded.py</figcaption>
        <pre>
          <code>
import queue
import threading

from algorithm import closest_pair, distance, generate_points

points = generate_points()

q = queue.Queue()
x1 = threading.Thread(
    target=lambda *args: q.put(closest_pair(*args)),
    args=(points[:int(len(points)/2)], points),
)
x2 = threading.Thread(
    target=lambda *args: q.put(closest_pair(*args)),
    args=(points[int(len(points)/2):], points),
)
x1.start()
x2.start()
x1.join()
x2.join()

res1 = q.get()
res2 = q.get()

closest = min([res1, res2], key=lambda res: distance(res[0], res[1]))
print(closest)
          </code>
        </pre>
      </figure>
      <figure>
        <figcaption>multi_threaded.py results</figcaption>
        <pre>
          <code>
$ time python multi_thread1.py
((7, 42), (8, 42))

real	0m18.996s
user	0m18.710s
sys	0m0.135s
          </code>
        </pre>
      </figure>
      <p>As expected, there was no speed up with multiple threads.</p>
      <h2>Multi-process performance</h2>
      <p>
        Our final move is using multiple processes. The multi-threading code
        above can be adapted with minimal changes to make it use multiple
        processes instead.
      </p>
      <figure>
        <figcaption>multi_process.py</figcaption>
        <pre>
          <code>
from multiprocessing import Process, Queue
import threading

from algorithm import closest_pair, distance, generate_points

points = generate_points()

q = Queue()

x1 = Process(
    target=lambda *args: q.put(closest_pair(*args)),
    args=(points[:int(len(points)/2)], points),
)
x2 = Process(
    target=lambda *args: q.put(closest_pair(*args)),
    args=(points[int(len(points)/2):], points),
)
x1.start()
x2.start()
x1.join()
x2.join()

res1 = q.get()
res2 = q.get()

closest = min([res1, res2], key=lambda res: distance(res[0], res[1]))
print(closest)
          </code>
        </pre>
      </figure>
      <figure>
        <figcaption>multi_process.py results</figcaption>
        <pre>
          <code>
$ time python multi_process.py
((7, 42), (8, 42))

real	0m9.647s
user	0m19.009s
sys	0m0.073s
          </code>
        </pre>
      </figure>
      <p>
        Here we see the expected, ideal 2x speed up by splitting the work across
        two processes.
      </p>
    </div>
  </body>
</html>
