<html>
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>A Python Concurrency Tutorial</title>
  </head>
  <body>
    <style>
      .content {
        max-width: 700px;
        margin: auto;
      }
    </style>
    <div class="content">
      <h1>
        Dependency Direction and Plugin Architecture in Python (aka Python3EE)
      </h1>
      <p>By <a href="/">Edward D'Souza</a> (June 14th, 2020)</p>
      <hr />
      <p>
        When I came to Python from Java, I reveled in the terseness and
        flexiblity of Python. However, as I'm reading through "Clean
        Architecture" by Robert C. Martin, I'm seeing that some of the
        enterprisey ideas from Java are actually quite useful with building
        large systems in team-envrionments. In this artcle I look at how
        dependency direction and interfaces can be used to create a plugin
        architecture in Python.
      </p>
      <p>
        While the ideas might seem like over engineering in the toy example
        here, I think they would be valuable in more realistic contexts with
        more complicated code and when you have multiple people or teams working
        together.
      </p>
      <p>
        (The example here and some code is taken from "Clean Architecture".)
      </p>
      <h1>Sections</h1>
      <ul>
        <li><a href="#example-problem">Example problem</a></li>
        <li><a href="#basic-algorithm">Basic Solution</a></li>
        <li><a href="#single-thread">Enterprise Edition</a></li>
      </ul>
      <h2 id="example-problem">Example Problem</h2>
      <p>
        The example problem we'll use is that you're writing an encrypter
        program. It has to take in characters from stdin, encrypt them using a
        translation table, and write the output to stdout.
      </p>
      <p>
        First, the easy part. The functional data-manipulation component that
        simple translates a character to its encrypted form using a shift
        cipher.
      </p>
      <figure>
        <figcaption>translate.py</figcaption>
        <pre>
          <code>
def translate(char: str):
    shift = 1
    letters = string.ascii_lowercase
    if char in letters:
        return letters[(letters.index(char) + shift) % len(letters)]
    return char
          </code>
        </pre>
      </figure>
      <h3 id="basic-algorithm">Basic Solution</h3>
      <p>
        Here is a straight-forward solution. It's probably what I would write by
        default when faced with the above requirements.
      </p>
      <p>
        To make things more flexible later, we're going to structure it as a
        function that takes in two sets of points ("from" and "to") and finds
        the closest pair of points between the two sets. To find the closest
        pair within an entire set of points, we can pass in the same set of
        points to both arguments.
      </p>
      <figure>
        <figcaption>main.py</figcaption>
        <pre>
          <code>
from algorithm import closest_pair

points = [(0,1), (9, 10), (2, 3)]
print(closest_pair(points, points))  # --> ((0, 1), (2, 3))
          </code>
        </pre>
      </figure>
      <figure>
        <figcaption>algorithm.py</figcaption>
        <pre>
          <code>

          </code>
        </pre>
      </figure>
      <h2 id="single-thread">Single-threaded performance</h2>
      <p>
        As a baseline, let's look at the how long it takes the algorithm to
        complete using a single thread. Note that I'm using NUM_POINTS = 3500
        for all the examples to make the problem sizable enough to measure on my
        machine.
      </p>
      <figure>
        <figcaption>single_thread1.py</figcaption>
        <pre>
          <code>
from algorithm import closest_pair, generate_points

points = generate_points()
closest = closest_pair(points, points)
print(closest)
          </code>
        </pre>
      </figure>
      <figure>
        <figcaption>single_thread1.py results</figcaption>
        <pre>
          <code>
$ time python single_thread1.py
((49, 97), (49, 96))

real    0m18.904s
user    0m18.665s
sys     0m0.056s
          </code>
        </pre>
      </figure>
      <h2 id="multi-thread">Multi-threaded performance</h2>
      <p>
        In a CPU bound task, there shouldn't be any benefit to using multiple
        threads, because of the
        <a
          href="https://docs.python.org/3/glossary.html#term-global-interpreter-lock"
          >GIL</a
        >. But we're going to do it anyway just to see what happens.
      </p>
      <figure>
        <figcaption>multi_thread1.py</figcaption>
        <pre>
          <code>

          </code>
        </pre>
      </figure>
      <figure>
        <figcaption>multi_thread1.py results</figcaption>
        <pre>
          <code>
$ time python multi_thread1.py
((7, 42), (8, 42))

real	0m18.996s
user	0m18.710s
sys	0m0.135s
          </code>
        </pre>
      </figure>
      <p>As expected, there was no speed up with multiple threads.</p>
      <h2 id="multi-proc">Multi-process performance</h2>
      <p>
        Our final move is using multiple processes. The multi-threading code
        above can be adapted with minimal changes to make it use multiple
        processes instead.
      </p>
      <h3 id="basic-multi-process">Initial Solution</h3>
      <figure>
        <figcaption>multi_proc1.py</figcaption>
        <pre>
          <code>
from multiprocessing import Process, Queue
import threading

from algorithm import closest_pair, distance, generate_points

points = generate_points()

q = Queue()

x1 = Process(
    target=lambda *args: q.put(closest_pair(*args)),
    args=(points[:len(points) // 2], points),
)
x2 = Process(
    target=lambda *args: q.put(closest_pair(*args)),
    args=(points[len(points) // 2:], points),
)
x1.start()
x2.start()
x1.join()
x2.join()

res1 = q.get()
res2 = q.get()

closest = min([res1, res2], key=lambda res: distance(res[0], res[1]))
print(closest)
          </code>
        </pre>
      </figure>
      <figure>
        <figcaption>multi_proc1.py results</figcaption>
        <pre>
          <code>
$ time python multi_process.py
((7, 42), (8, 42))

real	0m9.647s
user	0m19.009s
sys	0m0.073s
          </code>
        </pre>
      </figure>
      <p>
        Here we see the expected, ideal 2x speed up by splitting the work across
        two processes.
      </p>
      <h3 id="maxing-parallelism">Maxing out the parallelism</h3>
      <p>
        We can clean up the code a little with multiprocessing.Pool and bump the
        level of parallelism to match the system's CPU count.
      </p>
      <figure>
        <figcaption>multi_proc2.py</figcaption>
        <pre>
          <code>
import itertools
import multiprocessing

from algorithm import closest_pair, distance, generate_points


def segment_list(l, N):
    chunk_size, remainder = divmod(len(l), N)
    first, rest = l[:chunk_size + remainder], l[chunk_size + remainder:]
    return itertools.chain([first], zip(*[iter(rest)] * chunk_size))


points = generate_points()

N = multiprocessing.cpu_count()  # 8 for me.
segments = segment_list(points, N)

def do_it(segment):
    return closest_pair(segment, points)

pool = multiprocessing.Pool(processes=N)
results = pool.map(do_it, segments)

closest = min(results, key=lambda res: distance(res[0], res[1]))
print(closest)
          </code>
        </pre>
      </figure>
      <figure>
        <figcaption>multi_proc2.py results</figcaption>
        <pre>
          <code>
$ time python multi_proc2.py
((49, 97), (49, 96))

real	0m5.277s
user	0m38.252s
sys	0m0.168s
          </code>
        </pre>
      </figure>
      <p>
        Note that I only have 4 cores. "cpu_count" returns 8 on my system due to
        hyperthreading, but since I'm executing the exact same function on all
        processes, I probably don't see much benefit beyond 4 parallel tasks.
      </p>
      <p>
        Given all that, reducing total execution time to ~25% of the single
        threaded solution is pretty good!
      </p>
    </div>
  </body>
</html>
